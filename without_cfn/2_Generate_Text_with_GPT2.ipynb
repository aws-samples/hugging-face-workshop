{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with Pretrained GPT2 models from Hugging Face on Amazon SageMaker\n",
    "## The Poetry of NLP\n",
    "\n",
    "You’ve just been hired by the Chicago Tribune to start a new poetry column. Congrats! The catch? You need to write a new poem every day. And it can’t just be any old string of syllables, you need it to be fresh, authentic, to resonate with the times and carry a sense of rhyme. You need it to delight your readers, to drive up the Tribune’s daily readership numbers and grow their social media presence. How are you going to accomplish this? With the help of Hugging Face and NLP models on SageMaker of course! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook, we'll execute the following steps.\n",
    "\n",
    "1. Use the Hugging Face transformfers SDK to download pretrained NLP models and test them locally.\n",
    "2. Select a dataset from among our favorite authors.\n",
    "3. Finetune the pretrained model using SageMaker training.\n",
    "4. Deploy the model into S3.\n",
    "5. Trigger a pipeline to test and deploy the model onto a multi-container endpoint.\n",
    "6. Test your multi-model endpoint locally to write poetry and text in the style of your favorite authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, this notebook was built on SageMaker Studio, using an ml.t3.medium kernel gatway application, and the Python 3.6 PyTorch 1.8 CPU Jupyter Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Install the transformers SDK locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "\n",
    "transformers==4.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "     |████████████████████████████████| 2.2 MB 7.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (0.0.47)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (4.61.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (4.8.3)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (2022.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (1.19.1)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r requirements.txt (line 2)) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r requirements.txt (line 2)) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 2)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 2)) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 2)) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 2)) (1.0.1)\n",
      "Installing collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.4.0\n",
      "    Uninstalling huggingface-hub-0.4.0:\n",
      "      Successfully uninstalled huggingface-hub-0.4.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.3\n",
      "    Uninstalling transformers-4.12.3:\n",
      "      Successfully uninstalled transformers-4.12.3\n",
      "Successfully installed huggingface-hub-0.0.8 transformers-4.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Download a pretrained GPT2 model and test locally.\n",
    "We're using the Transformers SDK syntax available on the model card here: https://huggingface.co/gpt2 \n",
    "\n",
    "To make this model even better, we'll use a version of GPT2 that **has already been finetuned to generate poetry!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "poem_gpt = \"ismaelfaro/gpt2-poems.en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(poem_gpt)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(poem_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "def get_outputs(sample_outputs, tokenizer):\n",
    "    # takes a tokenizer, and raw output from the model, decodes these and formats nicely\n",
    "    rt = []\n",
    "\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        txt = tokenizer.decode(sample_output, skip_special_tokens = True)\n",
    "        print(\"{}: {}...\".format(i, txt))\n",
    "        print('')\n",
    "        rt.append(txt)\n",
    "        \n",
    "    return rt\n",
    "\n",
    "# setting the seed helps us ensure reproducibility. when the seed is consistent, we know the model results will be consistent\n",
    "set_seed(42)\n",
    "\n",
    "text = \"A rose by any other name\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors = 'pt')\n",
    "\n",
    "sample_outputs = base_model.generate(input_ids,\n",
    "                                do_sample = True, \n",
    "                                max_length = 70,\n",
    "                               num_return_sequences = 5)                              \n",
    "\n",
    "generic_outputs = get_outputs(sample_outputs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting and entertaining! Clearly this model knows the form of poetry. It is obviously generating short lines, with a newline, and it seems to pick up some interesting concepts. Now, let's see if we can fine-tune this poem writer to fit the style of an author we have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Fine-tune the GPT2 Poem model with Anne Bradstreet.\n",
    "Now, we're going to fine-tune this model using another, much smaller, dataset. Then later we'll use a text classifier trained to evaluate this style of writer, and see how well our new text performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious, take a look at some of the top authors in the English language available through this open domain site.\n",
    "https://www.public-domain-poetry.com/topauthors.php \n",
    "\n",
    "For the purposes of this workshop we'll stick to the longer poem pasted below. On your time time, outside of the workshop, if you'd like to modify this to work with a different text you are welcome to do so.\n",
    "\n",
    "Poke around at some of the available poems, and copy and paste what you like into this `train.txt` file below. We'll format that for finetuning GPT2 in the next step. In this notebook we're using a poem from Anne Bradstreet, a North American writer from the 17th Century.\n",
    "\n",
    "You may not have known this, but Anne Bradstreet was the first writer to be published in the North America!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.txt\n",
    "\n",
    "A Dialogue Between Old England And New\n",
    "\n",
    "    By Anne Bradstreet\n",
    "\n",
    "    New England.\n",
    "\n",
    "    Alas, dear Mother, fairest Queen and best,\n",
    "    With honour, wealth, and peace happy and blest,\n",
    "    What ails thee hang thy head, and cross thine arms,\n",
    "    And sit i� the dust to sigh these sad alarms?\n",
    "    What deluge of new woes thus over-whelm\n",
    "    The glories of thy ever famous Realm?\n",
    "    What means this wailing tone, this mournful guise?\n",
    "    Ah, tell thy Daughter; she may sympathize.\n",
    "\n",
    "    Old England.\n",
    "\n",
    "    Art ignorant indeed of these my woes,\n",
    "    Or must my forced tongue these griefs disclose,\n",
    "    And must my self dissect my tatter�d state,\n",
    "    Which Amazed Christendom stands wondering at?\n",
    "    And thou a child, a Limb, and dost not feel\n",
    "    My weak�ned fainting body now to reel?\n",
    "    This physic-purging-potion I have taken\n",
    "    Will bring Consumption or an Ague quaking,\n",
    "    Unless some Cordial thou fetch from high,\n",
    "    Which present help may ease my malady.\n",
    "    If I decease, dost think thou shalt survive?\n",
    "    Or by my wasting state dost think to thrive?\n",
    "    Then weigh our case, if �t be not justly sad.\n",
    "    Let me lament alone, while thou art glad.\n",
    "\n",
    "    New England.\n",
    "\n",
    "    And thus, alas, your state you much deplore\n",
    "    In general terms, but will not say wherefore.\n",
    "    What Medicine shall I seek to cure this woe,\n",
    "    If th� wound�s so dangerous, I may not know?\n",
    "    But you, perhaps, would have me guess it out.\n",
    "    What, hath some Hengist like that Saxon stout\n",
    "    By fraud and force usurp�d thy flow�ring crown,\n",
    "    Or by tempestuous Wars thy fields trod down?\n",
    "    Or hath Canutus, that brave valiant Dane,\n",
    "    The regal peaceful Sceptre from thee ta�en?\n",
    "    Or is �t a Norman whose victorious hand\n",
    "    With English blood bedews thy conquered Land?\n",
    "    Or is �t intestine Wars that thus offend?\n",
    "    Do Maud and Stephen for the Crown contend?\n",
    "    Do Barons rise and side against their King,\n",
    "    And call in Foreign aid to help the thing?\n",
    "    Must Edward be depos�d? Or is �t the hour\n",
    "    That second Richard must be clapp�d i� th� Tower?\n",
    "    Or is it the fatal jar, again begun,\n",
    "    That from the red, white pricking Roses sprung?\n",
    "    Must Richmond�s aid the Nobles now implore\n",
    "    To come and break the tushes of the Boar?\n",
    "    If none of these, dear Mother, what�s your woe?\n",
    "    Pray, do not fear Spain�s bragging Armado.\n",
    "    Doth your Ally, fair France, conspire your wrack,\n",
    "    Or doth the Scots play false behind your back?\n",
    "    Doth Holland quit you ill for all your love?\n",
    "    Whence is this storm, from Earth or Heaven above?\n",
    "    Is �t drought, is �t Famine, or is �t Pestilence?\n",
    "    Dost feel the smart, or fear the consequence?\n",
    "    Your humble Child entreats you shew your grief.\n",
    "    Though Arms nor Purse she hath for your relief�\n",
    "    Such is her poverty,�yet shall be found\n",
    "    A suppliant for your help, as she is bound.\n",
    "\n",
    "    Old England.\n",
    "\n",
    "    I must confess some of those Sores you name\n",
    "    My beauteous Body at this present maim,\n",
    "    But foreign Foe nor feigned friend I fear,\n",
    "    For they have work enough, thou knowest, elsewhere.\n",
    "    Nor is it Alcie�s son and Henry�s Daughter\n",
    "    Whose proud contention cause this slaughter;\n",
    "    Nor Nobles siding to make John no King,\n",
    "    French Louis unjustly to the Crown to bring;\n",
    "    No Edward, Richard, to lose rule and life,\n",
    "    Nor no Lancastrians to renew old strife;\n",
    "    No Crook-backt Tyrant now usurps the Seat,\n",
    "    Whose tearing tusks did wound, and kill, and threat.\n",
    "    No Duke of York nor Earl of March to soil\n",
    "    Their hands in Kindred�s blood whom they did foil;\n",
    "    No need of Tudor Roses to unite:\n",
    "    None knows which is the Red or which the White.\n",
    "    Spain�s braving Fleet a second time is sunk.\n",
    "    France knows how of my fury she hath drunk\n",
    "    By Edward third and Henry fifth of fame;\n",
    "    Her Lilies in my Arms avouch the same.\n",
    "    My Sister Scotland hurts me now no more,\n",
    "    Though she hath been injurious heretofore.\n",
    "    What Holland is, I am in some suspense,\n",
    "    But trust not much unto his Excellence.\n",
    "    For wants, sure some I feel, but more I fear;\n",
    "    And for the Pestilence, who knows how near?\n",
    "    Famine and Plague, two sisters of the Sword,\n",
    "    Destruction to a Land doth soon afford.\n",
    "    They�re for my punishments ordain�d on high,\n",
    "    Unless thy tears prevent it speedily.\n",
    "    But yet I answer not what you demand\n",
    "    To shew the grievance of my troubled Land.\n",
    "    Before I tell the effect I�ll shew the cause,\n",
    "    Which are my sins�the breach of sacred Laws:\n",
    "    Idolatry, supplanter of a N ation,\n",
    "    With foolish superstitious adoration,\n",
    "    Are lik�d and countenanc�d by men of might,\n",
    "    The Gospel is trod down and hath no right.\n",
    "    Church Offices are sold and bought for gain\n",
    "    That Pope had hope to find Rome here again.\n",
    "    For Oaths and Blasphemies did ever ear\n",
    "    From Beelzebub himself such language hear?\n",
    "    What scorning of the Saints of the most high!\n",
    "    What injuries did daily on them lie!\n",
    "    What false reports, what nick-names did they take,\n",
    "    Not for their own, but for their Master�s sake!\n",
    "    And thou, poor soul, wast jeer�d among the rest;\n",
    "    Thy flying for the Truth I made a jest.\n",
    "    For Sabbath-breaking and for Drunkenness\n",
    "    Did ever Land profaneness more express?\n",
    "    From crying bloods yet cleansed am not I,\n",
    "    Martyrs and others dying causelessly.\n",
    "    How many Princely heads on blocks laid down\n",
    "    For nought but title to a fading Crown!\n",
    "    �Mongst all the cruelties which I have done,\n",
    "    Oh, Edward�s Babes, and Clarence�s hapless Son,\n",
    "    O Jane, why didst thou die in flow�ring prime?�\n",
    "    Because of Royal Stem, that was thy crime.\n",
    "    For Bribery, Adultery, for Thefts, and Lies\n",
    "    Where is the Nation I can�t paralyze?\n",
    "    With Usury, Extortion, and Oppression,\n",
    "    These be the Hydras of my stout transgression;\n",
    "    These be the bitter fountains, heads, and roots\n",
    "    Whence flow�d the source, the sprigs, the boughs, and fruits.\n",
    "    Of more than thou canst hear or I relate,\n",
    "    That with high hand I still did perpetrate,\n",
    "    For these were threat�ned the woeful day\n",
    "    I mocked the Preachers, put it fair away.\n",
    "    The Sermons yet upon record do stand\n",
    "    That cried destruction to my wicked Land.\n",
    "    These Prophets� mouths (all the while) was stopt,\n",
    "    Unworthily, some backs whipt, and ears crept;\n",
    "    Their reverent cheeks bear the glorious marks\n",
    "    Of stinking, stigmatizing Romish Clerks;\n",
    "    Some lost their livings, some in prison pent,\n",
    "    Some grossly fined, from friends to exile went:\n",
    "    Their silent tongues to heaven did vengeance cry,\n",
    "    Who heard their cause, and wrongs judg�d righteously,\n",
    "    And will repay it sevenfold in my lap.\n",
    "    This is fore-runner of my after-clap.\n",
    "    Nor took I warning by my neighbors� falls.\n",
    "    I saw sad Germany�s dismantled walls,\n",
    "    I saw her people famish�d, Nobles slain,\n",
    "    Her fruitful land a barren heath remain.\n",
    "    I saw (unmov�d) her Armies foil�d and fled,\n",
    "    Wives forc�d, babes toss�d, her houses calcined.\n",
    "    I saw strong Rochelle yield�d to her foe,\n",
    "    Thousands of starved Christians there also.\n",
    "    I saw poor Ireland bleeding out her last,\n",
    "    Such cruelty as all reports have past.\n",
    "    Mine heart obdurate stood not yet aghast.\n",
    "    Now sip I of that cup, and just �t may be\n",
    "    The bottom dregs reserved are for me.\n",
    "\n",
    "    New England.\n",
    "\n",
    "    To all you�ve said, sad mother, I assent.\n",
    "    Your fearful sins great cause there �s to lament.\n",
    "    My guilty hands (in part) hold up with you,\n",
    "    A sharer in your punishment�s my due.\n",
    "    But all you say amounts to this effect,\n",
    "    Not what you feel, but what you do expect.\n",
    "    Pray, in plain terms, what is your present grief?\n",
    "    Then let�s join heads and hands for your relief.\n",
    "\n",
    "    Old England.\n",
    "\n",
    "    Well, to the matter, then. There�s grown of late\n",
    "    �Twixt King and Peers a question of state:\n",
    "    Which is the chief, the law, or else the King?\n",
    "    One saith, it�s he; the other, no such thing.\n",
    "    My better part in Court of Parliament\n",
    "    To ease my groaning land shew their intent\n",
    "    To crush the proud, and right to each man deal,\n",
    "    To help the Church, and stay the Common-Weal.\n",
    "    So many obstacles comes in their way\n",
    "    As puts me to a stand what I should say.\n",
    "    Old customs, new Prerogatives stood on.\n",
    "    Had they not held law fast, all had been gone,\n",
    "    Which by their prudence stood them in such stead\n",
    "    They took high Strafford lower by the head,\n",
    "    And to their Laud be �t spoke they held �n th� Tower\n",
    "    All England�s metropolitan that hour.\n",
    "    This done, an Act they would have passed fain\n",
    "    No prelate should his Bishopric retain.\n",
    "    Here tugg�d they hard indeed, for all men saw\n",
    "    This must be done by Gospel, not by law.\n",
    "    Next the Militia they urged sore.\n",
    "    This was denied, I need not say wherefore.\n",
    "    The King, displeased, at York himself absents.\n",
    "    They humbly beg return, shew their intents.\n",
    "    The writing, printing, posting to and fro,\n",
    "    Shews all was done; I�ll therefore let it go.\n",
    "    But now I come to speak of my disaster.\n",
    "    Contention�s grown �twixt Subjects and their Master,\n",
    "    They worded it so long they fell to blows,\n",
    "    That thousands lay on heaps. Here bleeds my woes.\n",
    "    I that no wars so many years have known\n",
    "    Am now destroy�d and slaughter�d by mine own.\n",
    "    But could the field alone this strife decide,\n",
    "    One battle, two, or three I might abide,\n",
    "    But these may be beginnings of more woe�\n",
    "    Who knows, the worst, the best may overthrow!\n",
    "    Religion, Gospel, here lies at the stake,\n",
    "    Pray now, dear child, for sacred Zion�s sake,\n",
    "    Oh, pity me in this sad perturbation,\n",
    "    My plundered Towns, my houses� devastation,\n",
    "    My ravisht virgins, and my young men slain,\n",
    "    My wealthy trading fallen, my dearth of grain.\n",
    "    The seedtime�s come, but Ploughman hath no hope\n",
    "    Because he knows not who shall inn his crop.\n",
    "    The poor they want their pay, their children bread,\n",
    "    Their woful mothers� tears unpitied.\n",
    "    If any pity in thy heart remain,\n",
    "    Or any child-like love thou dost retain,\n",
    "    For my relief now use thy utmost skill,\n",
    "    And recompense me good for all my ill.\n",
    "\n",
    "    New England.\n",
    "\n",
    "    Dear mother, cease complaints, and wipe your eyes,\n",
    "    Shake off your dust, cheer up, and now arise.\n",
    "    You are my mother, nurse, I once your flesh,\n",
    "    Your sunken bowels gladly would refresh.\n",
    "    Your griefs I pity much but should do wrong,\n",
    "    To weep for that we both have pray�d for long,\n",
    "    To see these latter days of hop�d-for good,\n",
    "    That Right may have its right, though �t be with blood.\n",
    "    After dark Popery the day did clear;\n",
    "    But now the Sun in�s brightness shall appear.\n",
    "    Blest be the Nobles of thy Noble Land\n",
    "    With (ventur�d lives) for truth�s defence that stand.\n",
    "    Blest be thy Commons, who for Common good\n",
    "    And thy infringed Laws have boldly stood.\n",
    "    Blest be thy Counties, who do aid thee still\n",
    "    With hearts and states to testify their will.\n",
    "    Blest be thy Preachers, who do cheer thee on.\n",
    "    Oh, cry: the sword of God and Gideon!\n",
    "    And shall I not on them wish Mero�s curse\n",
    "    That help thee not with prayers, arms, and purse?\n",
    "    And for my self, let miseries abound\n",
    "    If mindless of thy state I e�er be found.\n",
    "    These are the days the Church�s foes to crush,\n",
    "    To root out Prelates, head, tail, branch, and rush.\n",
    "    Let�s bring Baal�s vestments out, to make a fire,\n",
    "    Their Mitres, Surplices, and all their tire,\n",
    "    Copes, Rochets, Croziers, and such trash,\n",
    "    And let their names consume, but let the flash\n",
    "    Light Christendom, and all the world to see\n",
    "    We hate Rome�s Whore, with all her trumpery.\n",
    "    Go on, brave Essex, shew whose son thou art,\n",
    "    Not false to King, nor Country in thy heart,\n",
    "    But those that hurt his people and his Crown,\n",
    "    By force expel, destroy, and tread them down.\n",
    "    Let Gaols be fill�d with th� remnant of that pack,\n",
    "    And sturdy Tyburn loaded till it crack.\n",
    "    And ye brave Nobles, chase away all fear,\n",
    "    And to this blessed Cause closely adhere.\n",
    "    O mother, can you weep and have such Peers?\n",
    "    When they are gone, then drown your self in tears,\n",
    "    If now you weep so much, that then no more\n",
    "    The briny Ocean will o�erflow your shore.\n",
    "    These, these are they (I trust) with Charles our king,\n",
    "    Out of all mists such glorious days will bring\n",
    "    That dazzled eyes, beholding, much shall wonder\n",
    "    At that thy settled Peace, thy wealth, and splendour,\n",
    "    Thy Church and Weal establish�d in such manner\n",
    "    That all shall joy that thou display�dst thy banner,\n",
    "    And discipline erected so, I trust,\n",
    "    That nursing Kings shall come and lick thy dust.\n",
    "    Then Justice shall in all thy Courts take place\n",
    "    Without respect of persons or of case.\n",
    "    Then bribes shall cease, and suits shall not stick long,\n",
    "    Patience and purse of Clients for to wrong.\n",
    "    Then High Commissions shall fall to decay,\n",
    "    And Pursuivants and Catchpoles want their pay.\n",
    "    So shall thy happy Nation ever flourish,\n",
    "    When truth and righteousness they thus shall nourish.\n",
    "    When thus in Peace, thine Armies brave send out\n",
    "    To sack proud Rome, and all her vassals rout.\n",
    "    There let thy name, thy fame, and valour shine,\n",
    "    As did thine Ancestors� in Palestine,\n",
    "    And let her spoils full pay with int�rest be\n",
    "    Of what unjustly once she poll�d from thee.\n",
    "    Of all the woes thou canst let her be sped,\n",
    "    Execute to th� full the vengeance threatened.\n",
    "    Bring forth the beast that rul�d the world with�s beck,\n",
    "    And tear his flesh, and set your feet on�s neck,\n",
    "    And make his filthy den so desolate\n",
    "    To th� �stonishment of all that knew his state.\n",
    "    This done, with brandish�d swords to Turkey go,�\n",
    "    (For then what is it but English blades dare do?)\n",
    "    And lay her waste, for so�s the sacred doom,\n",
    "    And do to Gog as thou hast done to Rome.\n",
    "    Oh Abraham�s seed, lift up your heads on high,\n",
    "    For sure the day of your redemption�s nigh.\n",
    "    The scales shall fall from your long blinded eyes,\n",
    "    And him you shall adore who now despise.\n",
    "    Then fullness of the Nations in shall flow,\n",
    "    And Jew and Gentile to one worship go.\n",
    "    Then follows days of happiness and rest.\n",
    "    Whose lot doth fall to live therein is blest.\n",
    "    No Canaanite shall then be found �n th� land,\n",
    "    And holiness on horses� bells shall stand.\n",
    "    If this make way thereto, then sigh no more,\n",
    "    But if at all thou didst not see �t before.\n",
    "    Farewell, dear mother; Parliament, prevail,\n",
    "    And in a while you�ll tell another tale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Format your training data for Hugging Face on Amazon SageMaker.\n",
    "Now, let's parse your training data to format it for finetuning GPT2 and training on Hugging Face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    for row in f.readlines():\n",
    "        d = row.strip()\n",
    "        if len(d) > 1:\n",
    "            data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 304 valid objects in the training data.\n"
     ]
    }
   ],
   "source": [
    "print ('Found {} valid objects in the training data.'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Dialogue Between Old England And New', 'By Anne Bradstreet', 'New England.', 'Alas, dear Mother, fairest Queen and best,', 'With honour, wealth, and peace happy and blest,', 'What ails thee hang thy head, and cross thine arms,', 'And sit i� the dust to sigh these sad alarms?', 'What deluge of new woes thus over-whelm', 'The glories of thy ever famous Realm?', 'What means this wailing tone, this mournful guise?']\n"
     ]
    }
   ],
   "source": [
    "print (data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train.txt to s3://sagemaker-us-west-2-074535574140/gpt2/train.txt\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() \n",
    "\n",
    "train_file_name = 'train.txt'\n",
    "s3_train_data = 's3://{}/gpt2/{}'.format(bucket, train_file_name)\n",
    "\n",
    "!aws s3 cp {train_file_name} {s3_train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-13 18:30:06 Starting - Starting the training job......\n",
      "2022-03-13 18:30:43 Starting - Preparing the instances for training......\n",
      "2022-03-13 18:31:59 Downloading - Downloading input data\n",
      "2022-03-13 18:31:59 Training - Downloading the training image....................................\n",
      "2022-03-13 18:38:06 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-03-13 18:38:09,727 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-03-13 18:38:09,749 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-03-13 18:38:09,755 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-03-13 18:38:10,238 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.1.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (5.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets>=1.1.3->-r requirements.txt (line 1)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets>=1.1.3->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-03-13 18:38:13,212 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_training_compiler_debug_mode\": false,\n",
      "        \"sagemaker_training_compiler_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_train\": true,\n",
      "        \"model_name_or_path\": \"ismaelfaro/gpt2-poems.en\",\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 64,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.txt\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-trcomp-training-2022-03-13-18-29-53-098\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-074535574140/huggingface-pytorch-trcomp-training-2022-03-13-18-29-53-098/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_train\":true,\"model_name_or_path\":\"ismaelfaro/gpt2-poems.en\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":64,\"train_file\":\"/opt/ml/input/data/train/train.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_training_compiler_debug_mode\":false,\"sagemaker_training_compiler_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-074535574140/huggingface-pytorch-trcomp-training-2022-03-13-18-29-53-098/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_training_compiler_debug_mode\":false,\"sagemaker_training_compiler_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_train\":true,\"model_name_or_path\":\"ismaelfaro/gpt2-poems.en\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":64,\"train_file\":\"/opt/ml/input/data/train/train.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-trcomp-training-2022-03-13-18-29-53-098\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-074535574140/huggingface-pytorch-trcomp-training-2022-03-13-18-29-53-098/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_train\",\"True\",\"--model_name_or_path\",\"ismaelfaro/gpt2-poems.en\",\"--num_train_epochs\",\"5\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"64\",\"--train_file\",\"/opt/ml/input/data/train/train.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=ismaelfaro/gpt2-poems.en\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.txt\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 run_clm.py --do_train True --model_name_or_path ismaelfaro/gpt2-poems.en --num_train_epochs 5 --output_dir /opt/ml/model --per_device_train_batch_size 64 --train_file /opt/ml/input/data/train/train.txt\u001b[0m\n",
      "\u001b[34m[2022-03-13 18:38:14.916 torch.__training_compiler__.TrainingCompilerConfig INFO] Found configuration for Training Compiler. Compiler will be configured during import of torch_xla.\u001b[0m\n",
      "\u001b[34m[2022-03-13 18:38:16.988 torch_xla.__training_compiler__.TrainingCompilerConfig INFO] Configuring SM Training Compiler... \u001b[0m\n",
      "\u001b[34m[2022-03-13 18:38:16.989 torch_xla.__training_compiler__.TrainingCompilerConfig WARNING] Found pre-existing configuration GPU_NUM_DEVICES=1.\u001b[0m\n",
      "\u001b[34m03/13/2022 18:38:25 - WARNING - __main__ -   Process rank: -1, device: xla:1, n_gpu: 0distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m03/13/2022 18:38:25 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=0,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Mar13_18-38-19_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=5.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=64,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m03/13/2022 18:38:25 - WARNING - datasets.builder -   Using custom data configuration default-45cee01b36329d09\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-45cee01b36329d09/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 2542.00it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 903.94it/s]\u001b[0m\n",
      "\u001b[34m0 tables [00:00, ? tables/s]\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-45cee01b36329d09/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 129.60it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:25,941 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph1fxsxr0\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:25,941 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph1fxsxr0\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/927 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 927/927 [00:00<00:00, 165kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:26,258 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/baa271a09ddcd4ca545988075bac42eff4a39f455d505b828d1f3fd9b49160f2.3266e61f42db0eeb43a19f5aff2232f4bc20b7851a56506e5094df7f5decbf18\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:26,258 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/baa271a09ddcd4ca545988075bac42eff4a39f455d505b828d1f3fd9b49160f2.3266e61f42db0eeb43a19f5aff2232f4bc20b7851a56506e5094df7f5decbf18\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:26,258 >> creating metadata file for /root/.cache/huggingface/transformers/baa271a09ddcd4ca545988075bac42eff4a39f455d505b828d1f3fd9b49160f2.3266e61f42db0eeb43a19f5aff2232f4bc20b7851a56506e5094df7f5decbf18\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:26,258 >> creating metadata file for /root/.cache/huggingface/transformers/baa271a09ddcd4ca545988075bac42eff4a39f455d505b828d1f3fd9b49160f2.3266e61f42db0eeb43a19f5aff2232f4bc20b7851a56506e5094df7f5decbf18\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-03-13 18:38:26,259 >> loading configuration file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/baa271a09ddcd4ca545988075bac42eff4a39f455d505b828d1f3fd9b49160f2.3266e61f42db0eeb43a19f5aff2232f4bc20b7851a56506e5094df7f5decbf18\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-03-13 18:38:26,259 >> loading configuration file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/baa271a09ddcd4ca545988075bac42eff4a39f455d505b828d1f3fd9b49160f2.3266e61f42db0eeb43a19f5aff2232f4bc20b7851a56506e5094df7f5decbf18\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-03-13 18:38:26,264 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"ismaelfaro/gpt2-poems.en\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-03-13 18:38:26,264 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"ismaelfaro/gpt2-poems.en\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:26,581 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5fekt60n\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:26,581 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5fekt60n\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/256 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 256/256 [00:00<00:00, 208kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:26,891 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f3494c79c6acfbe2da92da375c46635316bb8589e5815b8fbea48f3133a19275.8d8340d30fe948c6a7e0297f442eb1117240ae561a831465f8b6819aeecd8e63\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:26,891 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f3494c79c6acfbe2da92da375c46635316bb8589e5815b8fbea48f3133a19275.8d8340d30fe948c6a7e0297f442eb1117240ae561a831465f8b6819aeecd8e63\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:26,891 >> creating metadata file for /root/.cache/huggingface/transformers/f3494c79c6acfbe2da92da375c46635316bb8589e5815b8fbea48f3133a19275.8d8340d30fe948c6a7e0297f442eb1117240ae561a831465f8b6819aeecd8e63\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:26,891 >> creating metadata file for /root/.cache/huggingface/transformers/f3494c79c6acfbe2da92da375c46635316bb8589e5815b8fbea48f3133a19275.8d8340d30fe948c6a7e0297f442eb1117240ae561a831465f8b6819aeecd8e63\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:27,472 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp28pr5i_w\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:27,472 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp28pr5i_w\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/779k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 90.0k/779k [00:00<00:00, 828kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 330k/779k [00:00<00:00, 1.47MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 779k/779k [00:00<00:00, 2.54MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:28,153 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fd2504741899012b475cb7099d3db388abad24f41dd390702f5fe25357ed46c2.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:28,153 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fd2504741899012b475cb7099d3db388abad24f41dd390702f5fe25357ed46c2.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:28,153 >> creating metadata file for /root/.cache/huggingface/transformers/fd2504741899012b475cb7099d3db388abad24f41dd390702f5fe25357ed46c2.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:28,153 >> creating metadata file for /root/.cache/huggingface/transformers/fd2504741899012b475cb7099d3db388abad24f41dd390702f5fe25357ed46c2.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:28,438 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl8voqmal\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:28,438 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl8voqmal\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 28.0k/446k [00:00<00:01, 244kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 188k/446k [00:00<00:00, 1.00MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 446k/446k [00:00<00:00, 1.54MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:29,001 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/d33f6e9a755a5a691199d1bfe7e2fd657694a2bad427b073c5472614b3d6fd12.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:29,001 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/d33f6e9a755a5a691199d1bfe7e2fd657694a2bad427b073c5472614b3d6fd12.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:29,001 >> creating metadata file for /root/.cache/huggingface/transformers/d33f6e9a755a5a691199d1bfe7e2fd657694a2bad427b073c5472614b3d6fd12.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:29,001 >> creating metadata file for /root/.cache/huggingface/transformers/d33f6e9a755a5a691199d1bfe7e2fd657694a2bad427b073c5472614b3d6fd12.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:29,298 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3zcg_suc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:29,298 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3zcg_suc\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 36.0k/1.29M [00:00<00:04, 278kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 182k/1.29M [00:00<00:01, 864kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 614k/1.29M [00:00<00:00, 2.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 3.05MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:30,049 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/1834be3e6dd323e7e5239aa87f723b40380306ba2473ae6e22b1c5763b496827.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:30,049 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/1834be3e6dd323e7e5239aa87f723b40380306ba2473ae6e22b1c5763b496827.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:30,049 >> creating metadata file for /root/.cache/huggingface/transformers/1834be3e6dd323e7e5239aa87f723b40380306ba2473ae6e22b1c5763b496827.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:30,049 >> creating metadata file for /root/.cache/huggingface/transformers/1834be3e6dd323e7e5239aa87f723b40380306ba2473ae6e22b1c5763b496827.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:30,643 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfp20zxth\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:30,643 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfp20zxth\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 90.0/90.0 [00:00<00:00, 98.8kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:30,938 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/3a70d24cc4b10db94dae7824e4418c0f3781f3c1451cd4044ce88c604b40b074.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:30,938 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/3a70d24cc4b10db94dae7824e4418c0f3781f3c1451cd4044ce88c604b40b074.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:30,938 >> creating metadata file for /root/.cache/huggingface/transformers/3a70d24cc4b10db94dae7824e4418c0f3781f3c1451cd4044ce88c604b40b074.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:30,938 >> creating metadata file for /root/.cache/huggingface/transformers/3a70d24cc4b10db94dae7824e4418c0f3781f3c1451cd4044ce88c604b40b074.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fd2504741899012b475cb7099d3db388abad24f41dd390702f5fe25357ed46c2.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/d33f6e9a755a5a691199d1bfe7e2fd657694a2bad427b073c5472614b3d6fd12.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/1834be3e6dd323e7e5239aa87f723b40380306ba2473ae6e22b1c5763b496827.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fd2504741899012b475cb7099d3db388abad24f41dd390702f5fe25357ed46c2.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/d33f6e9a755a5a691199d1bfe7e2fd657694a2bad427b073c5472614b3d6fd12.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/1834be3e6dd323e7e5239aa87f723b40380306ba2473ae6e22b1c5763b496827.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/3a70d24cc4b10db94dae7824e4418c0f3781f3c1451cd4044ce88c604b40b074.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f3494c79c6acfbe2da92da375c46635316bb8589e5815b8fbea48f3133a19275.8d8340d30fe948c6a7e0297f442eb1117240ae561a831465f8b6819aeecd8e63\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/3a70d24cc4b10db94dae7824e4418c0f3781f3c1451cd4044ce88c604b40b074.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-03-13 18:38:31,236 >> loading file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f3494c79c6acfbe2da92da375c46635316bb8589e5815b8fbea48f3133a19275.8d8340d30fe948c6a7e0297f442eb1117240ae561a831465f8b6819aeecd8e63\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:31,635 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8mi1djek\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-03-13 18:38:31,635 >> https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8mi1djek\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/487M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 3.80M/487M [00:00<00:12, 39.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 8.57M/487M [00:00<00:10, 45.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 13.9M/487M [00:00<00:09, 50.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 20.2M/487M [00:00<00:08, 56.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 26.4M/487M [00:00<00:08, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 32.1M/487M [00:00<00:07, 59.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 38.4M/487M [00:00<00:07, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 44.6M/487M [00:00<00:07, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 50.6M/487M [00:00<00:07, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 56.6M/487M [00:01<00:07, 59.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 62.2M/487M [00:01<00:07, 56.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 67.7M/487M [00:01<00:07, 55.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 73.4M/487M [00:01<00:07, 56.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 79.7M/487M [00:01<00:07, 59.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 85.9M/487M [00:01<00:06, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 92.0M/487M [00:01<00:06, 61.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 98.2M/487M [00:01<00:06, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██▏       | 104M/487M [00:01<00:06, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 111M/487M [00:01<00:06, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 117M/487M [00:02<00:06, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 123M/487M [00:02<00:05, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 130M/487M [00:02<00:05, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 136M/487M [00:02<00:05, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 142M/487M [00:02<00:05, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 149M/487M [00:02<00:06, 52.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 155M/487M [00:02<00:06, 55.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 161M/487M [00:02<00:05, 58.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 167M/487M [00:02<00:05, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 174M/487M [00:03<00:05, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 180M/487M [00:03<00:05, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 186M/487M [00:03<00:04, 63.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 192M/487M [00:03<00:04, 63.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 199M/487M [00:03<00:04, 64.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 205M/487M [00:03<00:04, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 211M/487M [00:03<00:04, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 218M/487M [00:03<00:04, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 224M/487M [00:03<00:05, 50.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 229M/487M [00:04<00:05, 50.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 235M/487M [00:04<00:04, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 241M/487M [00:04<00:04, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 247M/487M [00:04<00:04, 58.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 253M/487M [00:04<00:04, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 259M/487M [00:04<00:03, 61.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 265M/487M [00:04<00:03, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 272M/487M [00:04<00:03, 61.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 278M/487M [00:04<00:03, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 284M/487M [00:04<00:03, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 290M/487M [00:05<00:03, 64.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 297M/487M [00:05<00:03, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 303M/487M [00:05<00:03, 54.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 309M/487M [00:05<00:03, 57.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 315M/487M [00:05<00:03, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 321M/487M [00:05<00:02, 60.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 327M/487M [00:05<00:02, 61.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 333M/487M [00:05<00:02, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 340M/487M [00:05<00:02, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 346M/487M [00:06<00:02, 63.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 352M/487M [00:06<00:02, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 358M/487M [00:06<00:02, 61.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 364M/487M [00:06<00:02, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 371M/487M [00:06<00:01, 63.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 377M/487M [00:06<00:01, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 383M/487M [00:06<00:01, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 389M/487M [00:06<00:01, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 395M/487M [00:06<00:01, 64.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 402M/487M [00:06<00:01, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 408M/487M [00:07<00:01, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 414M/487M [00:07<00:01, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▋ | 420M/487M [00:07<00:01, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 427M/487M [00:07<00:00, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 433M/487M [00:07<00:00, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 439M/487M [00:07<00:00, 65.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 446M/487M [00:07<00:00, 65.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 452M/487M [00:07<00:00, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 458M/487M [00:07<00:00, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 464M/487M [00:07<00:00, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 471M/487M [00:08<00:00, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 477M/487M [00:08<00:00, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 483M/487M [00:08<00:00, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 487M/487M [00:08<00:00, 61.5MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:39,991 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/ae17805a098db035bbf8d27d45a55f8596489c0d857eee764453264a65f39fb1.bcfc0a076bcc8809bcb9104d51dd1091f94751b93ac7aa7aaf92a4e9e2f8ee09\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-03-13 18:38:39,991 >> storing https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/ae17805a098db035bbf8d27d45a55f8596489c0d857eee764453264a65f39fb1.bcfc0a076bcc8809bcb9104d51dd1091f94751b93ac7aa7aaf92a4e9e2f8ee09\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:39,991 >> creating metadata file for /root/.cache/huggingface/transformers/ae17805a098db035bbf8d27d45a55f8596489c0d857eee764453264a65f39fb1.bcfc0a076bcc8809bcb9104d51dd1091f94751b93ac7aa7aaf92a4e9e2f8ee09\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-03-13 18:38:39,991 >> creating metadata file for /root/.cache/huggingface/transformers/ae17805a098db035bbf8d27d45a55f8596489c0d857eee764453264a65f39fb1.bcfc0a076bcc8809bcb9104d51dd1091f94751b93ac7aa7aaf92a4e9e2f8ee09\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1323] 2022-03-13 18:38:39,991 >> loading weights file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ae17805a098db035bbf8d27d45a55f8596489c0d857eee764453264a65f39fb1.bcfc0a076bcc8809bcb9104d51dd1091f94751b93ac7aa7aaf92a4e9e2f8ee09\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1323] 2022-03-13 18:38:39,991 >> loading weights file https://huggingface.co/ismaelfaro/gpt2-poems.en/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ae17805a098db035bbf8d27d45a55f8596489c0d857eee764453264a65f39fb1.bcfc0a076bcc8809bcb9104d51dd1091f94751b93ac7aa7aaf92a4e9e2f8ee09\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1588] 2022-03-13 18:38:42,081 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1596] 2022-03-13 18:38:42,081 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ismaelfaro/gpt2-poems.en.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1588] 2022-03-13 18:38:42,081 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1596] 2022-03-13 18:38:42,081 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ismaelfaro/gpt2-poems.en.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 22.20ba/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 79.44ba/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:427] 2022-03-13 18:38:43,118 >> Using XLA device\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:427] 2022-03-13 18:38:43,118 >> Using XLA device\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1217] 2022-03-13 18:38:43,128 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1217] 2022-03-13 18:38:43,128 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1218] 2022-03-13 18:38:43,128 >>   Num examples = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1219] 2022-03-13 18:38:43,128 >>   Num Epochs = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1218] 2022-03-13 18:38:43,128 >>   Num examples = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1219] 2022-03-13 18:38:43,128 >>   Num Epochs = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1220] 2022-03-13 18:38:43,128 >>   Instantaneous batch size per device = 64\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1221] 2022-03-13 18:38:43,128 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1222] 2022-03-13 18:38:43,128 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1223] 2022-03-13 18:38:43,128 >>   Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1220] 2022-03-13 18:38:43,128 >>   Instantaneous batch size per device = 64\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1221] 2022-03-13 18:38:43,128 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1222] 2022-03-13 18:38:43,128 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1223] 2022-03-13 18:38:43,128 >>   Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:00<00:02,  1.74it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:43<01:16, 25.35s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [01:23<01:04, 32.11s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [01:23<00:19, 19.61s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:24<00:00, 12.69s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1437] 2022-03-13 18:40:07,425 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1437] 2022-03-13 18:40:07,425 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 96.3795, 'train_samples_per_second': 0.208, 'train_steps_per_second': 0.052, 'train_loss': 5.271908950805664, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:36<00:00, 12.69s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [01:36<00:00, 19.28s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-03-13 18:40:19,509 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-03-13 18:40:19,509 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-03-13 18:40:19,510 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-03-13 18:40:19,510 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-03-13 18:40:57,860 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-03-13 18:40:57,860 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-03-13 18:40:57,861 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-03-13 18:40:57,861 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-03-13 18:40:57,861 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-03-13 18:40:57,861 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        5.0\n",
      "  train_loss               =     5.2719\n",
      "  train_runtime            = 0:01:36.37\n",
      "  train_samples            =          4\n",
      "  train_samples_per_second =      0.208\n",
      "  train_steps_per_second   =      0.052\u001b[0m\n",
      "\u001b[34m2022-03-13 18:40:58,967 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-03-13 18:41:08 Uploading - Uploading generated training model\n",
      "2022-03-13 18:42:38 Completed - Training job completed\n",
      "Training seconds: 649\n",
      "Billable seconds: 649\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# gets role for executing training job\n",
    "role = sagemaker.get_execution_role()\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':\"ismaelfaro/gpt2-poems.en\",\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'do_train':True,\n",
    "    'train_file': '/opt/ml/input/data/train/{}'.format(train_file_name),\n",
    "    'num_train_epochs': 5,\n",
    "    # set batch size to 22 if using SM training compiler\n",
    "    \"per_device_train_batch_size\": 64,\n",
    "    # add your remaining hyperparameters\n",
    "    # more info here https://github.com/huggingface/transformers/tree/v4.6.1/examples/pytorch/language-modeling\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_clm.py',\n",
    "    source_dir='./examples/pytorch/language-modeling',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.11.0',\n",
    "    pytorch_version='1.9.0',\n",
    "    py_version='py38',\n",
    "    hyperparameters = hyperparameters,\n",
    "    # pass the training compiler config to speed up your job\n",
    "    compiler_config=TrainingCompilerConfig(),\n",
    "    environment = {'GPU_NUM_DEVICES':'1'},\n",
    "    disable_profiler = True,\n",
    "    debugger_hook_config = False\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "# should take about 13 minutes to run on current settings\n",
    "huggingface_estimator.fit({'train':s3_train_data}, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Test your trained model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-03-13 18:42:38 Starting - Preparing the instances for training\n",
      "2022-03-13 18:42:38 Downloading - Downloading input data\n",
      "2022-03-13 18:42:38 Training - Training image download completed. Training in progress.\n",
      "2022-03-13 18:42:38 Uploading - Uploading generated training model\n",
      "2022-03-13 18:42:38 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# redefining if you need to restart your kernel \n",
    "#huggingface_estimator = HuggingFace.attach('<paste your training job name here>')\n",
    "\n",
    "s3_model_data = huggingface_estimator.model_data\n",
    "local_model_path = 'gpt2_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘gpt2_finetuned’: File exists\n",
      "download: s3://sagemaker-us-west-2-074535574140/huggingface-pytorch-trcomp-training-2022-03-13-18-29-53-098/output/model.tar.gz to gpt2_finetuned/model.tar.gz\n",
      "vocab.json\n",
      "train_results.json\n",
      "trainer_state.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "merges.txt\n",
      "special_tokens_map.json\n",
      "pytorch_model.bin\n",
      "all_results.json\n",
      "training_args.bin\n",
      "config.json\n"
     ]
    }
   ],
   "source": [
    "!mkdir {local_model_path}\n",
    "!aws s3 cp {s3_model_data} {local_model_path}\n",
    "!tar -xvf {local_model_path}/model.tar.gz -C {local_model_path}\n",
    "!rm {local_model_path}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# optional - rerun this if you need to restart your kernel. We are actually using the same tokenizer from before\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step to make sure we can run inference with this model locally\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: A rose by any other name  \\nFor a summer to fill  \\nBirds' wings shining clear  \\nSun shone clear day’s sun  \\nLilies' lacy gown  \\nGlistening yellow  \\nFleecy smell all around  \\nSpring seemed to come soon  \\n...\n",
      "\n",
      "1: A rose by any other name  \\nJust another name for me.  \\nA rose bloom colors all  \\nA blossom bloom colors mine  \\nI bloom everyday a rose  \\nThe blossom bloom my everyday  \\nA rose everyday a rose  \\nJust another name for me.  \\nSo...\n",
      "\n",
      "2: A rose by any other name  \\nSo much for praise,  \\nI've brought a flower  \\nFrom my garden nook,  \\nUpon the first opening strain,  \\nTo clap its sweet-breath,  \\nFill my heart with sweet-breath,  \\nLike the first...\n",
      "\n",
      "3: A rose by any other name  \\nHas no name of itself  \\nBecause that name is sweet in itself  \\nIf there be any  \\nAny out there who  \\nLove this rose, know it  \\nThe sun rise, the shade fair  \\nOne flower, sun or shade  \\nMore...\n",
      "\n",
      "4: A rose by any other name  \\nTo bloom red or green;  \\nIt rose or blossom in other ways,  \\nWhen thou or hert no more may find:  \\nSee how the dews from their leaves have grown,  \\nAnd the air with full freshening, moves in the show...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "text = \"A rose by any other name \"\n",
    "input_ids = tokenizer.encode(text, return_tensors = 'pt')\n",
    "\n",
    "sample_outputs = model.generate(input_ids,\n",
    "                                do_sample = True, \n",
    "                                max_length = 70,\n",
    "                               num_return_sequences = 5)                              \n",
    "    \n",
    "bradsteet_raw = get_outputs(sample_outputs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, it certainly looks different. Let's see if we can modify this output using different paramters to invoke the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: A rose by any other name  \\nJust because  \\nShe will die somedaySome may say it’s the beauty  \\nOf the natural born flower  \\nThat makes each human unique.  \\nSome may call it’s wealth  \\nBut that money can’t buy you happiness  \\...\n",
      "\n",
      "1: A rose by any other name  \\nWhen all of a sudden  \\nStands amid the dew  \\nA precious rose  \\nA rose this good so rare  \\nWhen one holds it  \\nIn her hands and holds it  \\nA rose this friendly  \\nWhen all of a sudden ...\n",
      "\n",
      "2: A rose by any other name  \\nLike a rgin in a temple  \\nOr a prince on a high throne  \\nHow could I be in such case  \\nThat I would have no fame or name  \\nIn such a world, a rgin-born  \\nAlmighty, I could not...\n",
      "\n",
      "3: A rose by any other name  \\nIn his life's long parade  \\nMay be beautiful  \\nAnd may be beautiful.  \\nThis is selfless foolishness  \\nThis is surrenderAnd if my body  \\nIs of no importance  \\nThen I will die.The rose I thought I had,...\n",
      "\n",
      "4: A rose by any other name  \\nWith petals like a rose  \\nSo beautiful  \\nYet there was a doubt  \\nOver who would win  \\nWhat fragrance would have remained  \\nWhen all was said and done  \\nFor he had no rose  \\nLike the one he had  \\...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = model.generate(input_ids, \n",
    "                             max_length=70,\n",
    "                             do_sample=True, \n",
    "                             # only pick tokens at and above this probability level\n",
    "                             top_p=0.85,\n",
    "                             # only pick from this many tokens\n",
    "                             top_k=200,\n",
    "                             num_return_sequences = 5)                              \n",
    "\n",
    "\n",
    "bradstreet_top_85 = get_outputs(sample_outputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Quite a difference - not all of these seem very much like Bradstreet, and just much more generic. Yet the logical coherence on some of them is strong. Let's try it again with a smaller top_k and top_p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: A rose by any other name  \\nLeaves a fragrance  \\nAs petals are.When the pink rose finally emerges  \\nfrom your chest, slide it inside!  \\nThat much is true.  \\nThe petals are there - inside.  \\nOnce, in lifetime.  \\nMy sister...\n",
      "\n",
      "1: A rose by any other name  \\nFor whose beauty I can make my own  \\nBut every sweet flower I sell,  \\nThe rest that grows a thorn  \\nMay not be for sale,  \\nThe truth must be heard,  \\nI'll die with care.A lady at the corner  \\...\n",
      "\n",
      "2: A rose by any other name  \\nStands for someone else to love  \\nSo many ways you could choose to use  \\nYour rose to your self a treasure  \\nTo last a lifetime  \\nYour choice- any other words  \\nNo matter how much you love or hate  \\nCan there be...\n",
      "\n",
      "3: A rose by any other name  \\nStands tall as a proud rose  \\nBut their sion is deceing  \\nThe truth is  \\nYou ought to be ashamed  \\nTo anyone who mistreats  \\nThis truth is  \\nBeauty is simply a pathetic  \\nNature is sick...\n",
      "\n",
      "4: A rose by any other name  \\nCan never be told  \\nOnly I can hear  \\nPassion sighs  \\nFor words of realty to share,  \\nFor you are my angel  \\nEach thorn a thorn,  \\nCh Merriment gleaned  \\nTogether as a beautiful...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = model.generate(input_ids, \n",
    "                             max_length=70,\n",
    "                             do_sample=True, \n",
    "                             # only pick tokens at and above this probability level\n",
    "                             top_p=0.95,\n",
    "                             # only pick from this many tokens\n",
    "                             top_k=110,\n",
    "                             num_return_sequences = 5)                              \n",
    "\n",
    "\n",
    "bradstreet_top_95 = get_outputs(sample_outputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting - under these terms the model seems even more generic. You can still pick up a hint of that very old English style of writing, and yet the social media base terms come even more to the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Load a Text Classifier to Quantify Our Generated Text\n",
    "Now, we're going to use another model from the HF Hub. This time it's a text classifier, built specifically to give a strong signal for whether or not our text seems like it's in the style of Anne Bradstreet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "anne_model_name = 'edubz/anne_bradstreet'\n",
    "\n",
    "anne_tokenizer = AutoTokenizer.from_pretrained(anne_model_name)\n",
    "\n",
    "anne_clf = AutoModelForSequenceClassification.from_pretrained(anne_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def invoke_locally(text, anne_clf, anne_tokenizer):\n",
    "    \n",
    "    input_ids = anne_tokenizer(text, return_tensors = 'pt')\n",
    "\n",
    "    output = anne_clf(**input_ids)\n",
    "\n",
    "    logits = output['logits'].detach().numpy().tolist()[0]\n",
    "\n",
    "    res = softmax(logits).tolist()\n",
    "\n",
    "    conf = max(res)\n",
    "\n",
    "    label = res.index(conf)\n",
    "    \n",
    "    if label == 0:\n",
    "        label_str = 'Not Anne'\n",
    "    elif label == 1:\n",
    "        label_str = 'Anne'\n",
    "    \n",
    "    return {'confidence': conf, 'label':label_str }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence': 0.9999701566864546, 'label': 'Anne'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_locally(\"Alas, dear Mother, fairest Queen and best\", anne_clf, anne_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence': 0.9185171263809941, 'label': 'Not Anne'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_locally(\"A rose by any other name\", anne_clf, anne_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence': 0.9998957695763042, 'label': 'Not Anne'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_locally(\"< paste your generated text here >\", anne_clf, anne_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run some tests of your own. Try different invocation parameters. What seems to get you the highest Anne scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Deploy your fine-tuned model onto a SageMaker multi-model endpoint\n",
    "Now, let's deploy this model onto SageMaker. In particular we will trigger a pipeline to update an existing multi-model endpoint, and then invoke our model from that endpoint. \n",
    "\n",
    "We'll also list all available models from that endpoint, and test out generating text with each of these. Who knows, maybe we'll stumble on something good enough for the Tribune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bradstreet_path = 'gpt2-bradstreet-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-bradstreet-model/tokenizer_config.json',\n",
       " 'gpt2-bradstreet-model/special_tokens_map.json',\n",
       " 'gpt2-bradstreet-model/vocab.json',\n",
       " 'gpt2-bradstreet-model/merges.txt',\n",
       " 'gpt2-bradstreet-model/added_tokens.json',\n",
       " 'gpt2-bradstreet-model/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('{}/'.format(bradstreet_path))\n",
    "tokenizer.save_pretrained('{}/'.format(bradstreet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile \n",
    "\n",
    "b_model_name = '{}.tar.gz'.format(bradstreet_path)\n",
    "\n",
    "with tarfile.open(b_model_name, 'w:gz') as f:\n",
    "    f.add('gpt2-model/',arcname='.')\n",
    "f.close()\n",
    "\n",
    "prefix = 'gpt2-hf-workshop/gpt2-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x root/root         0 2022-03-13 18:49 ./\n",
      "-rw-r--r-- root/root 510401385 2022-03-13 18:49 ./pytorch_model.bin\n",
      "-rw-r--r-- root/root       200 2022-03-13 18:49 ./tokenizer_config.json\n",
      "-rw-r--r-- root/root       833 2022-03-13 18:49 ./config.json\n",
      "-rw-r--r-- root/root    456356 2022-03-13 18:49 ./merges.txt\n",
      "-rw-r--r-- root/root        90 2022-03-13 18:49 ./special_tokens_map.json\n",
      "-rw-r--r-- root/root   1355270 2022-03-13 18:49 ./tokenizer.json\n",
      "-rw-r--r-- root/root    798156 2022-03-13 18:49 ./vocab.json\n"
     ]
    }
   ],
   "source": [
    "! tar -ztvf {b_model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./gpt2-bradstreet-model.tar.gz to s3://sagemaker-us-west-2-074535574140/gpt2-hf-workshop/gpt2-test/gpt2-bradstreet-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {b_model_name} s3://{bucket}/{prefix}/{b_model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Test your fine-tuned model on SageMaker multi-model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "endpoints = client.list_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in endpoints['Endpoints']:\n",
    "    name = e['EndpointName']\n",
    "    if 'mme' in name:\n",
    "        mme_name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(endpoint_name = name, sagemaker_session=sess)\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A rose by any other name will be placed in the middle of the floor in the bottom of the room and will always stand at the side or at the edges of the floor and will always be in line with the rest of the floor.\\n\\n'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first time you invoke a model on an MME it will take longer to respond b/c the model is being copied from S3\n",
    "predictor.predict({\"inputs\":'A rose by any other name'}, target_model=b_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A rose by any other name, such as the \"Crown of Sorrows\" rose of a royal palace, is a one of all a queen can do, from the very beginning and by any means, which could be attained: because if'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictor = point to MME predictor here \n",
    "predictor.predict({\"inputs\":'A rose by any other name'}, target_model=b_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Write poetry for the Chicago Tribune\n",
    "Now - select your favorite lines from each output from GPT, and pass it in to the model. Feel free to modify the parameters using kwargs. When you are finished, you can submit your poem to our GitHub workshop page!\n",
    "\n",
    "**Please note** every time you invoke a new model via MME AWS is copying the model artifact from S3 to the SageMaker endpoint. That means **expect a big time delay whenever you invoke a new model.** \n",
    "\n",
    "One way to get around that is with model compilation, ie running SageMaker Neo to decrease the size, and thereby the runtime, of that model.\n",
    "\n",
    "In the poem below, I manually copied my favorite line from each output of the model, and fed it in to the generator. I manually pasted all of my favorites into the markdown file you see below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My poem - A rose by any other model\n",
    "\n",
    "A rose by any other name has many meanings. <br />\n",
    "When all that has been presented to us is a form of exaggeration. <br />\n",
    "The language will not preserve. <br />\n",
    "However, the old idea of he who has no business vainly passing by without any other <br />\n",
    "Some unending mizzen, deceived and deceived, seems ever more absurd and likely to harm our time. <br />\n",
    "We tuck his back into the sea which is on the plain almost as soon as we lose sight of him. <br />\n",
    "A mariner shall pass. <br />\n",
    "And I may leave nothing to thee till thou return, for as I said, My hand am strong when thou shouldst require it. <br />\n",
    "This comes out of Kant\\'s conviction that we have nothing in our minds<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"A rose by any other name has many meanings. Though I have not tried it myself by any means. But it's been very clever by people who haven't done it myself or by fans who have loved it and don't have, like, good feelings about it.\\n\\nI think this is going to be a very exciting, interesting time for\"}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'A rose by any other name has many meanings.'\n",
    "predictor.predict({\"inputs\":text, \n",
    "                   'parameters':{'max_length':70,\n",
    "                             'do_sample':True, \n",
    "                             # only pick tokens at and above this probability level\n",
    "                             'top_p':0.99,\n",
    "                             # only pick from this many tokens\n",
    "                             'top_k':600}}, \n",
    "                  target_model=b_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'However, the old idea of he who has no business vainly passing by without any other means, says that a good Christian, just as has the former, \"is not blinded by his ignorance of his own own conscience,\" to make use or contrive of something, though he had no occasion to do so by the idea of it.\\n\\n'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'However, the old idea of he who has no business vainly passing by without any other means'\n",
    "\n",
    "predictor.predict({\"inputs\":text, \n",
    "                   'parameters':{'max_length':70,\n",
    "                             'do_sample':True, \n",
    "                             # only pick tokens at and above this probability level\n",
    "                             'top_p':0.99,\n",
    "                             # only pick from this many tokens\n",
    "                             'top_k':600}}, \n",
    "                  target_model=b_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A mariner shall pass them into yonder province, and if no ships, they shall show her permission to sail again; or if he shall not see the mariner that came into yonder state I demand his presence.\\n\\nIt is possible that, seeing the mariner, he may avoid capture for another year? I am aware of no'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'A mariner shall pass'\n",
    "\n",
    "predictor.predict({\"inputs\":text, \n",
    "                   'parameters':{'max_length':70,\n",
    "                             'do_sample':True, \n",
    "                             # only pick tokens at and above this probability level\n",
    "                             'top_p':0.99,\n",
    "                             # only pick from this many tokens\n",
    "                             'top_k':600}}, \n",
    "                  target_model=b_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A rose by any other model. The rose was truly a special scent.\\n\\nA moment later, Tang Tian walked over to his back, stepping toward Liu Bei who had asked him with a little smile. Tang Tian knew because of his youth that the quality of the rose was less than ordinary among the rose gills.\\n\\nIn order to'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'A rose by any other model'\n",
    "\n",
    "predictor.predict({\"inputs\":text, \n",
    "                   'parameters':{'max_length':70,\n",
    "                             'do_sample':True, \n",
    "                             # only pick tokens at and above this probability level\n",
    "                             'top_p':0.99,\n",
    "                             # only pick from this many tokens\n",
    "                             'top_k':100}}, \n",
    "                  target_model=b_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - use a pretrained GPTJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Endpoints notebook in this repository to deploy and test a GPT-J 6B endpoint. Compare the generation to that of your fine-tuned GPT-2 model. Add some of the lines to your poem if you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional- Use Other Pretrained Models and Hugging Face Datasets\n",
    "\n",
    "**Available pretrained models and datasets from Hugging Face**\n",
    "\n",
    "**Datasets**\n",
    "- Shakespeare:\n",
    "    - https://huggingface.co/datasets/tiny_shakespeare \n",
    "\n",
    "**Pretrained models**\n",
    "- Chinese Poetry:\n",
    "    - https://huggingface.co/caixin1998/chinese-poetry-gpt2 \n",
    "- Hebrew Poetry:\n",
    "    - https://huggingface.co/Norod78/hebrew_poetry-gpt_neo-small \n",
    "- Arabic Poetry:\n",
    "    - https://huggingface.co/akhooli/gpt2-small-arabic-poetry \n",
    "- Russian Poetry:\n",
    "    - https://huggingface.co/TuhinColumbia/russianpoetrymany \n",
    "- Persian Poetry:\n",
    "    - https://huggingface.co/mitra-mir/BERT-Persian-Poetry \n",
    "- Italian Poetry:\n",
    "     - https://huggingface.co/TuhinColumbia/italianpoetrymany "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Use Hugging Face and Text Generation on Amazon SageMaker for Your Organization\n",
    "Now that you've learned how to test, finetune, deploy and utilize a text generation model on SageMaker, let's understand how to apply that within your organzation.\n",
    "\n",
    "First, think to yourself, does my organization already produce a lot of written text? Do we write documentation, scripts, blog posts, documents, answers to questions, customer messaging, etc? Odds are, you do. \n",
    "\n",
    "Then, ask yourself, where do we already have a large volume of written text I can easily access? That may be your existing public documentation, your existing blog posts, etc. First, run through this notebook and use some of your own data to finetune a GPT model. See how well that performs, then consider scaling to large models, including GPT-J. \n",
    "\n",
    "If you really aren't seeing the performance you want, [consider training a model from scratch!](https://github.com/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb )\n",
    "\n",
    "Look at this and other examples within [Hugging Face's SageMaker example notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker), and similar examples on the [SageMaker repository!](https://github.com/aws/amazon-sagemaker-examples/search?q=hugging+face)\n",
    "\n",
    "Remember that in order to get the best performance we **combined a variety of computer-generated and human-discriminated approaches**. Further work could improve on this by training discriminator NLP models, such as text classifiers in certain styles, to make the generation and improvement process even faster."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
